#!/usr/bin/env python3
"""
GPUç’°å¢ƒãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

FuguMTç¿»è¨³ã‚µãƒ¼ãƒãƒ¼ã§åˆ©ç”¨å¯èƒ½ãªGPUç’°å¢ƒã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import sys
import platform


def check_python_environment():
    """Pythonç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ Pythonç’°å¢ƒ:")
    print(f"   ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    print(f"   ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {platform.platform()}")
    print()


def check_pytorch():
    """PyTorchç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ”¥ PyTorchç’°å¢ƒ:")
    
    try:
        import torch
        print(f"   PyTorch: {torch.__version__}")
        print(f"   CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"   CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.version.cuda}")
            gpu_count = torch.cuda.device_count()
            print(f"   GPUãƒ‡ãƒã‚¤ã‚¹æ•°: {gpu_count}")
            print()
            print("   åˆ©ç”¨å¯èƒ½ãªGPU:")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_properties = torch.cuda.get_device_properties(i)
                gpu_memory = gpu_properties.total_memory / (1024**3)
                compute_capability = f"{gpu_properties.major}.{gpu_properties.minor}"
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
                try:
                    torch.cuda.empty_cache()
                    allocated = torch.cuda.memory_allocated(i) / (1024**3)
                    cached = torch.cuda.memory_reserved(i) / (1024**3)
                    free = gpu_memory - cached
                    print(f"   GPU {i}: {gpu_name}")
                    print(f"      ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB (ä½¿ç”¨ä¸­: {cached:.1f}GB, ç©ºã: {free:.1f}GB)")
                    print(f"      Compute Capability: {compute_capability}")
                    print(f"      è¨­å®šä¾‹: device = cuda, gpu_id = {i}")
                except Exception:
                    print(f"   GPU {i}: {gpu_name}")
                    print(f"      ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB")
                    print(f"      Compute Capability: {compute_capability}")
                    print(f"      è¨­å®šä¾‹: device = cuda, gpu_id = {i}")
                print()
            
            if gpu_count > 1:
                print("   ğŸ” è¤‡æ•°GPUãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼")
                print("   è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§GPU IDã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ç‰¹å®šã®GPUã‚’ä½¿ç”¨ã§ãã¾ã™:")
                print("   config/fugumt_translator.ini ã® [TRANSLATION] ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§")
                print("   device = cuda")
                print("   gpu_id = 0  # ä½¿ç”¨ã—ãŸã„GPUã®IDï¼ˆ0ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰")
                print()
        
        # Apple Silicon (MPS) ãƒã‚§ãƒƒã‚¯
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("   MPSï¼ˆApple Siliconï¼‰: åˆ©ç”¨å¯èƒ½")
        
        print()
        
    except ImportError:
        print("   âŒ PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("      pip install torch ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print()


def check_transformers():
    """Transformersç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸ¤— Transformersç’°å¢ƒ:")
    
    try:
        import transformers
        print(f"   Transformers: {transformers.__version__}")
        print()
    except ImportError:
        print("   âŒ TransformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("      pip install transformers ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print()


def check_websockets():
    """WebSocketsç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("ğŸŒ WebSocketsç’°å¢ƒ:")
    
    try:
        import websockets
        print(f"   WebSockets: {websockets.__version__}")
        print()
    except ImportError:
        print("   âŒ WebSocketsãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("      pip install websockets ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        print()


def check_fugumt_model():
    """FuguMTãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ¡ FuguMTãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ:")
    
    try:
        from transformers import MarianTokenizer, MarianMTModel
        
        model_name = "staka/fugumt-en-ja"
        print(f"   ãƒ¢ãƒ‡ãƒ«: {model_name}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ
        print("   ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        print("   âœ… ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ãŸã‚æ³¨æ„ï¼‰
        print("   ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        model = MarianMTModel.from_pretrained(model_name)
        print("   âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        
        # ç°¡å˜ãªç¿»è¨³ãƒ†ã‚¹ãƒˆ
        print("   ç¿»è¨³ãƒ†ã‚¹ãƒˆä¸­...")
        inputs = tokenizer("Hello world", return_tensors="pt")
        outputs = model.generate(**inputs, max_length=50, num_beams=4)
        translated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"   ãƒ†ã‚¹ãƒˆç¿»è¨³: 'Hello world' -> '{translated}'")
        print("   âœ… ç¿»è¨³ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        print()
        
    except Exception as e:
        print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("   ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        print()


def check_system_resources():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹:")
    
    try:
        import psutil
        
        # CPUæƒ…å ±
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"   CPU: {cpu_count}ã‚³ã‚¢ (ä½¿ç”¨ç‡: {cpu_percent}%)")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        memory_percent = memory.percent
        print(f"   ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f}GB (ä½¿ç”¨ç‡: {memory_percent}%)")
        
        if memory_gb < 4:
            print("   âš ï¸  ãƒ¡ãƒ¢ãƒªãŒ4GBæœªæº€ã§ã™ã€‚å‹•ä½œãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        elif memory_gb < 8:
            print("   âš ï¸  ãƒ¡ãƒ¢ãƒªãŒ8GBæœªæº€ã§ã™ã€‚å¤§ããªãƒ¢ãƒ‡ãƒ«ã§å•é¡ŒãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        
        # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡
        disk = psutil.disk_usage('.')
        disk_gb = disk.free / (1024**3)
        print(f"   åˆ©ç”¨å¯èƒ½ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡: {disk_gb:.1f}GB")
        
        if disk_gb < 5:
            print("   âš ï¸  ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¿…è¦ã§ã™ã€‚")
        
        print()
        
    except ImportError:
        print("   psutilãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
        print("   pip install psutil ã§ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’ç¢ºèªã§ãã¾ã™")
        print()


def run_comprehensive_test():
    """ç·åˆãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ç·åˆå‹•ä½œãƒ†ã‚¹ãƒˆ:")
    
    try:
        # FuguMTTranslatorã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
        from FuguMTTranslator import Config, FuguMTTranslator
        
        print("   è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆ...")
        config = Config()
        print("   âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæˆåŠŸ")
        
        print("   ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ...")
        # æ³¨æ„: ã“ã‚Œã¯æ™‚é–“ãŒã‹ã‹ã‚Šãƒ¡ãƒ¢ãƒªã‚’å¤§é‡ã«ä½¿ç”¨ã—ã¾ã™
        print("   (ã“ã®å‡¦ç†ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...)")
        translator = FuguMTTranslator(config)
        print("   âœ… ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–æˆåŠŸ")
        
        print("   ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ...")
        health = translator.health_check()
        print(f"   ãƒ˜ãƒ«ã‚¹çŠ¶æ…‹: {health['status']}")
        print(f"   ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {health['device']}")
        
        # ä½¿ç”¨ä¸­ã®GPUè©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
        if health['device'].startswith('cuda'):
            import torch
            if ':' in health['device']:
                gpu_id = int(health['device'].split(':')[1])
            else:
                gpu_id = torch.cuda.current_device()
            
            gpu_name = torch.cuda.get_device_name(gpu_id)
            gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory / (1024**3)
            try:
                allocated = torch.cuda.memory_allocated(gpu_id) / (1024**3)
                print(f"   ä½¿ç”¨GPUè©³ç´°: {gpu_name} (ID: {gpu_id})")
                print(f"   GPU ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB (ç¾åœ¨ä½¿ç”¨ä¸­: {allocated:.1f}GB)")
            except Exception:
                print(f"   ä½¿ç”¨GPUè©³ç´°: {gpu_name} (ID: {gpu_id}, ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB)")
        
        print("   âœ… ç·åˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
        print()
        
    except Exception as e:
        print(f"   âŒ ç·åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print("   setup.py ã‚’å®Ÿè¡Œã—ã¦ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å®Œäº†ã—ã¦ãã ã•ã„")
        print()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*50)
    print("ğŸ” FuguMTç¿»è¨³ã‚µãƒ¼ãƒãƒ¼ ç’°å¢ƒãƒã‚§ãƒƒã‚¯")
    print("="*50)
    print()
    
    # åŸºæœ¬ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    check_python_environment()
    check_pytorch()
    check_transformers()
    check_websockets()
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
    check_system_resources()
    
    # FuguMTãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯
    check_fugumt_model()
    
    # ç·åˆãƒ†ã‚¹ãƒˆ
    run_comprehensive_test()
    
    print("="*50)
    print("âœ… ç’°å¢ƒãƒã‚§ãƒƒã‚¯å®Œäº†")
    print("="*50)
    print()
    print("æ¨å¥¨äº‹é …:")
    print("- GPUåˆ©ç”¨æ™‚ã¯ååˆ†ãªãƒ¡ãƒ¢ãƒªï¼ˆ8GBä»¥ä¸Šï¼‰ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„")
    print("- åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™")
    print("- å®‰å®šã—ãŸå‹•ä½œã«ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šãŒå¿…è¦ã§ã™")


if __name__ == "__main__":
    main()